"""
model.py

Full pipeline:
- train (optional)
- QAT fine-tune (optional/if supported)
- fuse conv+bn where possible
- export TorchScript
- convert to Core ML (mlprogram) and quantize weights to float16
"""

import os
import math
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.ao.quantization as tq
from torch.utils.data import DataLoader, Subset, WeightedRandomSampler
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split

DATA_DIR = "dataset-resized"
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 20                 
QAT_EPOCHS = 6               
MAX_LR = 1e-3
SEED = 42
MIXUP = True
MIXUP_ALPHA = 0.2
USE_SAMPLER = True
BEST_MODEL = "best_resnet_lite.pth"        
QAT_MODEL = "qat_resnetlite.pth"          
OUTPUT_TORCH = "resnetlite_traced.pt"
OUTPUT_COREML = "resnetlite_edge.mlpackage"  
NUM_WORKERS = 0

DO_TRAIN = False   
DO_QAT = True      
DO_EXPORT = True

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

train_transform = transforms.Compose([
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(12),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

def conv3x3(in_ch, out_ch, stride=1):
    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)

class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, in_ch, out_ch, stride=1, downsample=None):
        super().__init__()
        self.conv1 = conv3x3(in_ch, out_ch, stride)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(out_ch, out_ch)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)
        out = self.conv2(out); out = self.bn2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out

class ResNetLite(nn.Module):
    def __init__(self, block=BasicBlock, layers=[2,2,2,2], num_classes=6, base_ch=32):
        super().__init__()
        self.in_ch = base_ch
        self.conv1 = nn.Conv2d(3, base_ch, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(base_ch)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)

        self.layer1 = self._make_layer(block, base_ch, layers[0], stride=1)
        self.layer2 = self._make_layer(block, base_ch*2, layers[1], stride=2)
        self.layer3 = self._make_layer(block, base_ch*4, layers[2], stride=2)
        self.layer4 = self._make_layer(block, base_ch*8, layers[3], stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(base_ch*8*block.expansion, num_classes)

    def _make_layer(self, block, out_ch, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_ch != out_ch * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_ch, out_ch * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_ch * block.expansion)
            )
        layers = []
        layers.append(block(self.in_ch, out_ch, stride, downsample))
        self.in_ch = out_ch * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_ch, out_ch))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)
        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)
        x = self.avgpool(x); x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

def mixup_data(x, y, alpha=MIXUP_ALPHA):
    if alpha <= 0:
        return x, y, y, 1.0
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_loss(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def evaluate(model, loader, device):
    model.eval()
    total = correct = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            _, preds = out.max(1)
            total += y.size(0)
            correct += (preds == y).sum().item()
    return correct / total

def train_main(model, train_loader, val_loader, epochs, device, save_path):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=MAX_LR, weight_decay=1e-4)
    steps_per_epoch = math.ceil(len(train_loader.dataset) / BATCH_SIZE)
    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=MAX_LR, steps_per_epoch=steps_per_epoch, epochs=epochs)

    best_val = 0.0
    for epoch in range(1, epochs+1):
        model.train()
        running_loss = 0.0; total = 0; correct = 0
        for i, (images, labels) in enumerate(train_loader):
            images = images.to(device); labels = labels.to(device)
            if MIXUP:
                images, targets_a, targets_b, lam = mixup_data(images, labels)
                outputs = model(images)
                loss = mixup_loss(nn.CrossEntropyLoss(), outputs, targets_a, targets_b, lam)
            else:
                outputs = model(images)
                loss = nn.CrossEntropyLoss()(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            running_loss += loss.item() * images.size(0)
            _, preds = outputs.max(1)
            if MIXUP:
                correct += (preds == targets_a).sum().item()
            else:
                correct += (preds == labels).sum().item()
            total += labels.size(0)

        train_loss = running_loss / total
        train_acc = correct / total

        val_acc = evaluate(model, val_loader, device)
        print(f"Epoch {epoch:02d}/{epochs} - Train Loss: {train_loss:.4f} Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%")

        if val_acc > best_val:
            best_val = val_acc
            torch.save(model.state_dict(), save_path)
            print(f"  -> Saved best model to {save_path}")

    return best_val

def main():
    full_train = datasets.ImageFolder(DATA_DIR, transform=train_transform)
    full_val   = datasets.ImageFolder(DATA_DIR, transform=val_transform)
    classes = full_train.classes
    num_classes = len(classes)
    print("Classes:", classes, "Total images:", len(full_train))

    targets = np.array(full_train.targets)
    indices = np.arange(len(targets))
    train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=targets, random_state=SEED)
    train_ds = Subset(full_train, train_idx)
    val_ds   = Subset(full_val, val_idx)

    if USE_SAMPLER:
        train_targets = targets[train_idx]
        class_counts = np.bincount(train_targets, minlength=num_classes)
        class_weights = 1.0 / (class_counts + 1e-12)
        sample_weights = class_weights[train_targets]
        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)
        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS)
    else:
        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

    base_ch = 32   
    model = ResNetLite(num_classes=num_classes, base_ch=base_ch).to(DEVICE)

    if DO_TRAIN:
        print("Starting training...")
        train_main(model, train_loader, val_loader, EPOCHS, DEVICE, BEST_MODEL)
    else:
        if os.path.exists(BEST_MODEL):
            print("Loading weights from", BEST_MODEL)
            model.load_state_dict(torch.load(BEST_MODEL, map_location=DEVICE))
        else:
            raise FileNotFoundError(f"{BEST_MODEL} not found. Set DO_TRAIN=True to train or place the file.")

    # Evaluate baseline
    print("Evaluating baseline model...")
    baseline_acc = evaluate(model, val_loader, DEVICE)
    print(f"Baseline val accuracy: {baseline_acc*100:.2f}%")

    # -------- QAT flow (optional) --------
    if DO_QAT:
        print("Attempting QAT flow...")
        try:
            qconfig = tq.get_default_qat_qconfig("fbgemm")
        except Exception:
            try:
                qconfig = tq.get_default_qat_qconfig("qnnpack")
            except Exception:
                print("QAT qconfig not available on this build â€” skipping QAT.")
                qconfig = None

        if qconfig is not None:
            model_qat = ResNetLite(num_classes=num_classes, base_ch=base_ch)
            model_qat.load_state_dict(model.state_dict())
            model_qat.train()

            model_qat.qconfig = qconfig
            try:
                tq.prepare_qat(model_qat, inplace=True)
                print("Prepared model for QAT.")
                train_main(model_qat, train_loader, val_loader, QAT_EPOCHS, DEVICE, QAT_MODEL)
                model_qat.eval()
                tq.convert(model_qat, inplace=True)
                torch.save(model_qat.state_dict(), QAT_MODEL)
                print("QAT conversion complete, saved to", QAT_MODEL)
                model = model_qat
            except Exception as e:
                print("QAT failed with exception:", e)
                print("Proceeding with non-QAT model for export.")
        else:
            print("QAT qconfig unavailable; skipping QAT step.")


    try:
        fused_model = ResNetLite(num_classes=num_classes, base_ch=base_ch)
        fused_model.load_state_dict(model.state_dict())
        fused_model.train()
        fuse_list = []
        if hasattr(fused_model, "conv1") and hasattr(fused_model, "bn1"):
            fuse_list.append(["conv1", "bn1", "relu"])
            torch.quantization.fuse_modules(fused_model, fuse_list, inplace=True)
            print("Fused stem modules:", fuse_list)
        fused_model.eval()
    except Exception as e:
        print("Fusing failed/partial:", e)
        fused_model = model

  
    try:
        fused_model.to(memory_format=torch.channels_last)
        print("Model set to channels_last format for performance.")
    except Exception:
        pass

    example_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE).to(memory_format=torch.channels_last)
    traced = None
    try:
        traced = torch.jit.trace(fused_model.cpu(), example_input.cpu())
        torch.jit.save(traced, OUTPUT_TORCH)
        print("Saved TorchScript ->", OUTPUT_TORCH)
    except Exception as e:
        print("TorchScript trace failed:", e)
        raise

    if DO_EXPORT:
        import coremltools as ct
        from coremltools.models.neural_network import quantization_utils

        print("Converting to Core ML (mlprogram) ...")
        
        mlmodel = ct.convert(
            traced,
            inputs=[ct.ImageType(name="input_1", shape=(1, 3, IMG_SIZE, IMG_SIZE), scale=1/255.0)],
            convert_to="mlprogram",
            compute_units=ct.ComputeUnit.ALL,
            compute_precision=ct.precision.FLOAT16
        )
        mlmodel.save("adcnn_trashnet_fp16.mlpackage")
        print("Saved Core ML model:", "adcnn_trashnet_fp16.mlpackage")

    print("Pipeline finished.")

if __name__ == "__main__":
    main()
